---
title: Spatial Interaction with Spatial Dependence
jupyter: conda-env-nersa25-py
---



what if our flows are not independent?

@fischer2008ModelingSpatial, @lesage2007KnowledgeSpillovers, @lesage2008SpatialEconometric, @lesage2010SpatialEconometric, @lesage2013SpatialInteraction, @lesage2014WhatRegional, @thomas-agnan2014SpatialEconometric

```{python}
import geopandas as gpd
import numpy as np
import pandas as pd
from geosnap import DataStore
from geosnap import io as gio
from libpysal.graph import Graph
import scipy
from formulaic import Formula
from spreg import GMM_Error, GM_Lag
```

like any regression model, a critical assumption is that observations are *independent* from one another. And like any model using spatial data, the model is misspecified if residuals are spatially autocorrelated (indicating the input data fail the independence criterion). We can use spatial econometric approaches to handle this situation, albeit with some minor modifications because 

- autocorrelation may come from origins, destinations, or both
- we need to approximate the data using a log-linear model instead of proper Poisson

Approaches for estimating spatial lag models are described in @lesage2007KnowledgeSpillovers and @lesage2008SpatialEconometric while error models are decribed by @fischer2008ModelingSpatial, the latter two of which use conventional estimation techniques with specialized $W$ matrices based on the notion of neighboring origins or neighboring destinations.

## Demographic & Socioeconomic Data

### ACS Data

```{python}
datasets = DataStore()

dc = gio.get_acs(datasets, state_fips="11", years=2021, level="tract")
```

```{python}
dc.head()
```

```{python}
dc.plot()
```

### LODES Data

```{python}
dc_flows = pd.read_csv(
    "https://lehd.ces.census.gov/data/lodes/LODES8/dc/od/dc_od_main_JT00_2022.csv.gz",
    converters={"w_geocode": str, "h_geocode": str},
    low_memory=False,
    encoding="latin1",
)
```

```{python}
dc_flows
```

add tract-level geoids and drop extraneous columns

```{python}
dc_flows["w_tr_geocode"] = dc_flows["w_geocode"].str[:11]
dc_flows["h_tr_geocode"] = dc_flows["h_geocode"].str[:11]

dc_flows = dc_flows[["w_geocode", "h_geocode", "w_tr_geocode", "h_tr_geocode", "S000"]]
```

aggregate to tract-level flows (from block-level)

```{python}
dc_flows = (
    dc_flows.groupby(["w_tr_geocode", "h_tr_geocode"])["S000"].sum().reset_index()
)
```

this is an origin-destination matrix (as an adjacency list). `S000` is the number of flows between home census tract `h_tr_geocode` and work tract `w_tr_geocode`. There are many intra-tract flows $W_{ii}\neq0$} (people commuting within the same tract, or *possibly* WFH, though that would be tough with LODES) and the flows are *directed* $W_{ij} \neq W_{ji}$

```{python}
dc_flows
```

create a pysal Graph to represent these flows. to represent the direction of flow correctly the focal column is home and neighbor is work (this shows the AM commute)

```{python}
dc_flow_graph = Graph.from_adjacency(
    adjacency=dc_flows,
    focal_col="h_tr_geocode",
    neighbor_col="w_tr_geocode",
    weight_col="S000",
)
```

```{python}
dc_flow_graph.summary(asymmetries=True)
```

```{python}
23306 / 206**2
```

```{python}
dc_flow_graph.adjacency.head()
```

the `Graph` is ordered, and to visualize it correctly we want to align our other data to match this Graph; so stash the id/order

```{python}
idx = dc_flow_graph.unique_ids
```

create a geodataframe of centroids to visualize point-to-point flows, then reindex to re-order appropriately and drop observations with no flow.

```{python}
dc_centroids = dc.set_geometry(dc.centroid)
dc_centroids = dc_centroids.set_index("geoid").reindex(idx)
```

```{python}
dc_centroids.shape  # matches our Graph.n
```

```{python}
dc = dc.set_index("geoid")
```

```{python}
# for our dataset we want the full dense matrix
dc_interaction = pd.Series(dc_flow_graph.sparse.toarray().reshape(-1), 
                             index=pd.MultiIndex.from_product([dc_flow_graph.unique_ids, 
                                                               dc_flow_graph.unique_ids.rename('neighbor')])).rename('weight')
```

```{python}
dc_interaction = dc_interaction.reset_index()
dc_interaction
```

```{python}
dc.shape[0] ** 2
```

in spatial interaction terms focal=origin and neighbor=destination, so we can attach attributes of origins and destinations by doing left-joins on the y-variable successively based on focal, then neighbor

```{python}
# first merge origin attributes
dc_interaction = dc_interaction.merge(
    dc.drop(columns=["geometry"]), left_on="focal", right_index=True, how="left"
)
```

```{python}
# now merge destination attributes
dc_interaction = dc_interaction.merge(
    dc.drop(columns=["geometry"]),
    left_on="neighbor",
    right_index=True,
    how="left",
    suffixes=["_origin", "_destination"],
)
```

note the suffixes differentiate columns referring to origin and destination values, e.g. `median_household_income_origin` and `median_household_income_destination`

```{python}
dc_interaction
```

```{python}
dc_interaction[
    ["weight", "median_household_income_origin", "median_household_income_destination"]
]
```

for a spatial interaction model we also need *another* Graph that measures the distance between observations. There are many ways to do this, but the easiest is to use a distance band weight where all observations are guaranteed to lie within the threshold (giving us a pairwise matrix)

```{python}
dc = dc.to_crs(dc.estimate_utm_crs())
```

keep only tracts in the dataframe in our flow graph (origins), then get distance between observations with no decay

```{python}
dc = dc[dc.index.isin(dc_flow_graph.unique_ids)]

dc_dist = Graph.build_distance_band(
    dc.set_geometry(dc.centroid), threshold=1e20, binary=False, alpha=1
)
```

```{python}
dc_dist.summary()
```

```{python}
dc_dist.adjacency
```

```{python}
# subset the distance graph by the travel graph (remove destinations we dont need)
# but this resets weights to 1
dc_dist_adj = dc_dist.intersection(dc_flow_graph).adjacency
```

```{python}
# update with the old values
dc_dist_adj.update(dc_dist.adjacency)
```

```{python}
dc_interaction["distance"] = dc_dist.sparse.toarray().reshape(-1)
```

```{python}
dc_interaction['weight'] = dc_interaction['weight'].astype(int)
```

## Spatial Weights (Graphs) for OD-Flows

In this case observations are neighbors if:

- there is a flow between o and d
- if o_i shares a border with o_j

```{python}
contg = Graph.build_contiguity(dc)
```

```{python}
contg
```

```{python}
contg.explore(dc.centroid)
```

the `contg` Graph encodes flows as neighbors if the origin tracts share a border.

but we need to multiply that graph to get it into the correct dimensions to match our flow data. Following @lesage2008SpatialEconometric and @fischer2008ModelingSpatial we do this via a Kronecker product between our flow and contiguity graphs to create the $W$ used in the model.


take the Kronecker product of the two Graphs, then reinstantiate a new one

```{python}
kg = Graph.from_sparse(scipy.sparse.kron(dc_flow_graph.transform("b").sparse, contg.sparse))
```

our new graph now as the same length as our observation vector

```{python}
kg.n
```

```{python}
dc_interaction.shape[0]
```

this is an origin-centric ODW, so to get the destination-centric weights you'd do the transpose of the OD matrix (flow graph) first [@lesage2008SpatialEconometric]

```{python}
kg.pct_nonzero
```

```{python}
contg.pct_nonzero
```

```{python}
dc_flow_graph.pct_nonzero
```

row-standardize

```{python}
kg = kg.transform('r')
```

`spreg` will only treat the Graph as a matrix, so the ordering of the sparse representation is all that matters, not the indices/labels; i.e. the Graph has the correct shape and order even though the indices of the Graph are different than those of the observations

## Main Model Specification

> "Note in some cases yij = 0, indicating the absence of flows from i to j. This leads to the so-called zero problem,  since the logarithm then is undefined. There are several pragmatic solutions to this problem, with adding a  small constant to the zero elements of [yij ] being widely used. Here we added 0.08." [@fischer2008ModelingSpatial]

that gives our $y$ variable a distribution like this

```{python}
dc_interaction.weight.replace(0,0.08).apply(np.log).hist()
```

another common transformation is to add 1 to every observation and take the log of *that*, i.e. take $log(x+1)$

```{python}
dc_interaction.weight.apply(np.log1p).hist()
```


we specify a log-linear model using `formulaic` to generate our $y$ and $X$ matrices, then pass these to different kinds of spatial econometric models

```{python}
form = "np.log1p(weight) ~ 1+ np.log1p(n_total_pop_origin) + np.log1p(median_household_income_origin) + np.log1p(p_nonhisp_black_persons_origin) + np.log1p(n_total_pop_destination) + np.log1p(median_household_income_destination) + np.log1p(p_nonhisp_black_persons_destination) + np.log1p(distance)"
```

```{python}
f = Formula(form)

# mean-impute missing values and replace any zeros with 0.08 for convenience
y, x = f.get_model_matrix(
    dc_interaction.fillna(dc_interaction.mean(numeric_only=True))#.replace(0, 0.08)
)
```

```{python}
#| scrolled: true
y
```






## Spatial Lag

### Origin centric weights

```{python}
flow_lag = GM_Lag(y=y, x=x, w=kg, robust='white')
```

```{python}
#| scrolled: true
print(flow_lag.summary)
```

```{python}
flow_lag.output
```

```{python}
pd.Series(flow_lag.u.flatten()).hist()
```




### Destination-centric weights

```{python}
kgd = Graph.from_sparse(scipy.sparse.kron(dc_flow_graph.transform("b").sparse.transpose(), contg.sparse))
```

```{python}
kgd = kgd.transform('r')
```

```{python}
kgd.pct_nonzero
```

```{python}
dest_flow_lag = GM_Lag(y=y, x=x,  w=kgd)
```

```{python}
#| scrolled: true
print(dest_flow_lag.summary)
```

```{python}
dest_flow_lag.output
```


### OD-centric weights

one "OD-graph" could be the union of the two; a flow is 'neighbors' with another flow if it is contiguous with *either* origin or destination points

```{python}
kg_od = kg.transform('b').union(kgd.transform('b'))
```

```{python}
kg_od = kg_od.transform('r')
```

```{python}
kg_od.pct_nonzero
```

```{python}
od_flow_lag = GM_Lag(y=y, x=x, w=kg_od)
```

```{python}
print(od_flow_lag.summary)
```

```{python}
od_flow_lag.output
```



Instead, we could take the sum of the two graphs, in which case you are neighbors when contiguous with *either* origin or destination points (same cardinalities as above), but the strength of the weight is 2x if you neighbor *both* origin and destination.

```{python}
kg_od = Graph.from_sparse(kg.transform('b').sparse + kgd.transform('b').sparse)
kg_od = kg_od.transform('r')
```

```{python}
print(od_flow_lag.summary)
```

```{python}
od_flow_lag.output
```

## Spatial Error

the error models take a really long time to estimate

### Origin-Centric

```{python}
flow_error_origin = GMM_Error(y=y, x=x, w=kg)
```

```{python}
print(flow_error_origin.summary)
```

```{python}
flow_error_origin.output
```


### Destination-centric

```{python}
flow_error_dest = GMM_Error(y=y, x=x, w=kgd)
```

```{python}
print(flow_error_dest.summary)
```

```{python}
flow_error_dest.output
```


### OD-Centric

```{python}
flow_error_od = GMM_Error(y=y, x=x, w=kg_od)
```

```{python}
print(flow_error_od.summary)
```

```{python}
flow_error_od.output
```



## References


